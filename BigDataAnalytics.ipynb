{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-s4UZIX07Hd5",
    "outputId": "eb8b02c0-b635-4173-db65-0d7b5ffbe494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "| ProductId|count|\n",
      "+----------+-----+\n",
      "|B000FA398U|    1|\n",
      "|B00523NRVO|    3|\n",
      "|B002R7XYBQ|    1|\n",
      "|B001RIXUS8|    3|\n",
      "|B000JSQKNE|    2|\n",
      "|B004N71J7O|    3|\n",
      "|B000FKL0EU|   15|\n",
      "|B0080HFENI|    2|\n",
      "|B001EO62CG|    1|\n",
      "|B005P0NI4K|    1|\n",
      "|B001EQ4LAE|   11|\n",
      "|B0018CJYPG|   33|\n",
      "|B0052UOQY4|    1|\n",
      "|B000HEA95K|    9|\n",
      "|B00432EV3I|    6|\n",
      "|B0042RNHVG|    1|\n",
      "|B000YUOY30|   14|\n",
      "|B004FWYAYG|    6|\n",
      "|B001EQ5FQI|    6|\n",
      "|B00142IAKU|    8|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonReviewsEDA\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = spark.read.csv(\"/content/Reviews.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter data for reviews with scores >= 4\n",
    "filtered_data = data.filter(col(\"Score\") >= 4)\n",
    "\n",
    "# Aggregate reviews by ProductId and count\n",
    "aggregated_data = filtered_data.groupBy(\"ProductId\").count()\n",
    "\n",
    "# Show results\n",
    "aggregated_data.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQiJGXVJdsaV",
    "outputId": "b5fb13c6-456e-4093-81a5-16baec082c89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- ProductId: string (nullable = true)\n",
      " |-- UserId: string (nullable = true)\n",
      " |-- ProfileName: string (nullable = true)\n",
      " |-- HelpfulnessNumerator: string (nullable = true)\n",
      " |-- HelpfulnessDenominator: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Text: string (nullable = false)\n",
      " |-- label: double (nullable = true)\n",
      "\n",
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+-----+\n",
      "| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|label|\n",
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+-----+\n",
      "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|  4.0|\n",
      "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|  0.0|\n",
      "|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|  3.0|\n",
      "|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|  1.0|\n",
      "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|  4.0|\n",
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|  8.0|    16|\n",
      "|  0.0| 41764|\n",
      "|  7.0|     9|\n",
      "| -1.0|   890|\n",
      "| 64.0|     2|\n",
      "| NULL|   235|\n",
      "| 18.0|     3|\n",
      "|  1.0| 23539|\n",
      "|  4.0|282305|\n",
      "| 50.0|     2|\n",
      "| 56.0|     1|\n",
      "| 11.0|     3|\n",
      "| 21.0|     7|\n",
      "| 14.0|     6|\n",
      "| 22.0|     2|\n",
      "| 68.0|     2|\n",
      "|  3.0| 62904|\n",
      "| 59.0|     1|\n",
      "| 46.0|     9|\n",
      "| 28.0|     1|\n",
      "+-----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Training data count: 355958\n",
      "Test data count: 89307\n"
     ]
    }
   ],
   "source": [
    "# Print schema and sample data\n",
    "data.printSchema()\n",
    "data.show(5)\n",
    "\n",
    "# Drop nulls and fill empty values\n",
    "data = data.na.drop(subset=[\"Text\", \"Score\"])\n",
    "data = data.na.fill(\"\", subset=[\"Text\"])\n",
    "\n",
    "# Create label column\n",
    "data = data.withColumn(\"label\", col(\"Score\") - 1)\n",
    "\n",
    "# Check label distribution\n",
    "data.groupBy(\"label\").count().show()\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Check split counts\n",
    "print(f\"Training data count: {train_data.count()}\")\n",
    "print(f\"Test data count: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "biDl-2SYe86I",
    "outputId": "3855910a-412f-485a-8c62-5739d992f8d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[i, have, bought, several, of, the, vitality, canned, dog, food, products, and, have, found, them, all, to, be, of, good, quality., the, product, looks, more, like, a, stew, than, a, processed, meat, and, it, smells, better., my, labrador, is, finicky, and, she, appreciates, this, product, better, than, , most.]                                                                                                                                                      |\n",
      "|[\"product, arrived, labeled, as, jumbo, salted, peanuts...the, peanuts, were, actually, small, sized, unsalted., not, sure, if, this, was, an, error, or, if, the, vendor, intended, to, represent, the, product, as, \"\"jumbo\"\".\"]                                                                                                                                                                                                                                             |\n",
      "|[\"this, is, a, confection, that, has, been, around, a, few, centuries., , it, is, a, light,, pillowy, citrus, gelatin, with, nuts, -, in, this, case, filberts., and, it, is, cut, into, tiny, squares, and, then, liberally, coated, with, powdered, sugar., , and, it, is, a, tiny, mouthful, of, heaven., , not, too, chewy,, and, very, flavorful., , i, highly, recommend, this, yummy, treat., , if, you, are, familiar, with, the, story, of, c.s., lewis', \"\"the, lion]|\n",
      "|[if, you, are, looking, for, the, secret, ingredient, in, robitussin, i, believe, i, have, found, it., , i, got, this, in, addition, to, the, root, beer, extract, i, ordered, (which, was, good), and, made, some, cherry, soda., , the, flavor, is, very, medicinal.]                                                                                                                                                                                                        |\n",
      "|[great, taffy, at, a, great, price., , there, was, a, wide, assortment, of, yummy, taffy., , delivery, was, very, quick., , if, your, a, taffy, lover,, this, is, a, deal.]                                                                                                                                                                                                                                                                                                    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|filtered_words                                                                                                                                                                                                                                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[bought, several, vitality, canned, dog, food, products, found, good, quality., product, looks, like, stew, processed, meat, smells, better., labrador, finicky, appreciates, product, better, , most.]                                                                                          |\n",
      "|[\"product, arrived, labeled, jumbo, salted, peanuts...the, peanuts, actually, small, sized, unsalted., sure, error, vendor, intended, represent, product, \"\"jumbo\"\".\"]                                                                                                                           |\n",
      "|[\"this, confection, around, centuries., , light,, pillowy, citrus, gelatin, nuts, -, case, filberts., cut, tiny, squares, liberally, coated, powdered, sugar., , tiny, mouthful, heaven., , chewy,, flavorful., , highly, recommend, yummy, treat., , familiar, story, c.s., lewis', \"\"the, lion]|\n",
      "|[looking, secret, ingredient, robitussin, believe, found, it., , got, addition, root, beer, extract, ordered, (which, good), made, cherry, soda., , flavor, medicinal.]                                                                                                                          |\n",
      "|[great, taffy, great, price., , wide, assortment, yummy, taffy., , delivery, quick., , taffy, lover,, deal.]                                                                                                                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|raw_features                                                                                                                                                                                                                                                                                                |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(262144,[0,1,4,10,14,28,32,35,37,169,178,354,419,439,448,449,1023,1352,1893,4128,4232,6424,7499,12655],[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                   |\n",
      "|(262144,[10,90,110,132,221,1278,1618,1889,2396,2590,3046,5186,7901,11433,15184,34740,185825],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                         |\n",
      "|(262144,[0,19,59,153,180,288,393,410,546,573,593,743,793,1239,1689,1998,2415,2443,2473,2818,3006,3693,3724,4168,4972,7919,12546,12716,15732,35112,78237,80893,129241],[5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(262144,[0,13,17,35,40,44,76,118,372,569,695,796,838,1631,1642,2075,2691,3559,11613,16960,55767],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                     |\n",
      "|(262144,[0,5,274,651,793,1213,2362,2724,4041,4238,5076,14083],[3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])                                                                                                                                                                                            |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"words\")\n",
    "tokenized_data = tokenizer.transform(data)\n",
    "tokenized_data.select(\"words\").show(5, truncate=False)\n",
    "\n",
    "# StopWordsRemover\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_data = stopwords_remover.transform(tokenized_data)\n",
    "filtered_data.select(\"filtered_words\").show(5, truncate=False)\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
    "try:\n",
    "    vectorizer_model = vectorizer.fit(filtered_data)\n",
    "    vectorized_data = vectorizer_model.transform(filtered_data)\n",
    "    vectorized_data.select(\"raw_features\").show(5, truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Vectorizer Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f4qcJFCdIvW",
    "outputId": "08c50b64-9a04-444e-af31-4c39627e9e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: An error occurred while calling o579.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 69.0 failed 1 times, most recent failure: Lost task 1.0 in stage 69.0 (TID 85) (2384d3c32d3c executor driver): java.lang.RuntimeException: Labels MUST be in [0, 2147483647), but got -1.0\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\n",
      "\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:517)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.RuntimeException: Labels MUST be in [0, 2147483647), but got -1.0\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(vectorized_data)\n",
    "transformed_data = idf_model.transform(vectorized_data)\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, vectorizer, idf, lr])\n",
    "\n",
    "# Train the model\n",
    "if train_data.count() > 0:\n",
    "    try:\n",
    "        model = pipeline.fit(train_data)\n",
    "        predictions = model.transform(test_data)\n",
    "\n",
    "        # Evaluate the model\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "        print(f\"Model Accuracy: {accuracy}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training Error: {e}\")\n",
    "else:\n",
    "    print(\"Training data is empty. Please check your dataset or filtering logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wBKkk607JI-"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Data preprocessing\n",
    "tokenizer = Tokenizer(inputCol=\"review_body\", outputCol=\"words\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Train-test split\n",
    "data = data.withColumn(\"label\", data[\"star_rating\"] - 1)  # Assuming star ratings are 1-5\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Model pipeline\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, vectorizer, idf, lr])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Test the model\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYyTzFYQ7JMG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHcktpya7JPU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdhBW_Yx7JUC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IBJ2TXi7JXJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ka0-XMYH7Jaa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
